"""
Internal LLM Interface and Implementations
ë³´ì•ˆ ì•„í‚¤í…ì²˜: ë‚´ë¶€ ë¯¼ê° ë°ì´í„° ì „ìš© LLM

Phase 1 (MVP): ì™¸ë¶€ API ì‚¬ìš© (GPT-3.5, Claude Haiku) + ì¸í„°í˜ì´ìŠ¤ ì¶”ìƒí™”
Phase 2 (Pilot): Private Cloud LLM (Azure OpenAI, AWS Bedrock)
Phase 3 (Production): On-Premise LLM (Llama 3, Solar)
"""

import json
import logging
import os
import time
from abc import ABC, abstractmethod
from enum import Enum
from typing import Optional, Any

import litellm
from litellm import completion

from app.core.config import settings
from app.worker.llm.exceptions import (
    AllProvidersFailedError,
    InvalidResponseError,
)

logger = logging.getLogger(__name__)


class InternalLLMProvider(Enum):
    """Internal LLM ì œê³µì (êµì²´ ê°€ëŠ¥)"""
    MVP_OPENAI = "mvp_openai"          # Phase 1: GPT-3.5-turbo (ì €ë¹„ìš©)
    MVP_ANTHROPIC = "mvp_anthropic"    # Phase 1: Claude 3 Haiku (ê²½ëŸ‰)
    AZURE_OPENAI = "azure_openai"      # Phase 2: Azure OpenAI (êµ­ë‚´ ë¦¬ì „)
    AWS_BEDROCK = "aws_bedrock"        # Phase 2: AWS Bedrock Claude
    ONPREM_LLAMA = "onprem_llama"      # Phase 3: Llama 3 (On-Premise)
    ONPREM_SOLAR = "onprem_solar"      # Phase 3: Upstage Solar


class DataClassification(Enum):
    """ë°ì´í„° ë¶„ë¥˜"""
    PUBLIC = "PUBLIC"           # ê³µê°œ ë°ì´í„° (ë‰´ìŠ¤, ê³µì‹œ)
    INTERNAL = "INTERNAL"       # ë‚´ë¶€ ë¯¼ê° ë°ì´í„° (KYC, ì—¬ì‹ )
    SEMI_PUBLIC = "SEMI_PUBLIC" # ì¤€ê³µê°œ (ê³µì‹œ ì¬ë¬´ì œí‘œ vs ì œì¶œ ì¬ë¬´ì œí‘œ)


class InternalLLMBase(ABC):
    """
    Internal LLM ì¶”ìƒ ì¸í„°í˜ì´ìŠ¤

    ëª¨ë“  Internal LLM êµ¬í˜„ì²´ëŠ” ì´ ì¸í„°í˜ì´ìŠ¤ë¥¼ ë”°ë¦„
    â†’ Phase ì „í™˜ ì‹œ êµ¬í˜„ì²´ë§Œ êµì²´í•˜ë©´ ë¨

    ğŸ”’ ë³´ì•ˆ ìš”êµ¬ì‚¬í•­:
    - ë°ì´í„°ê°€ ì€í–‰ ë„¤íŠ¸ì›Œí¬ ì™¸ë¶€ë¡œ ë‚˜ê°€ì§€ ì•ŠìŒ (Phase 3)
    - ëª¨ë“  ì²˜ë¦¬ ë¡œê·¸ ê°ì‚¬ ì¶”ì  ê°€ëŠ¥
    - ê°œì¸ì‹ë³„ì •ë³´(PII) ë§ˆìŠ¤í‚¹ ì˜µì…˜
    """

    @property
    @abstractmethod
    def provider(self) -> InternalLLMProvider:
        """í˜„ì¬ Provider ë°˜í™˜"""
        pass

    @property
    @abstractmethod
    def model_name(self) -> str:
        """ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ ì´ë¦„"""
        pass

    @abstractmethod
    def analyze_snapshot(self, snapshot_json: dict) -> dict:
        """
        ë‚´ë¶€ Snapshot ë¶„ì„ â†’ Direct Signal í›„ë³´ ìƒì„±

        Args:
            snapshot_json: PRD 7ì¥ ìŠ¤í‚¤ë§ˆì˜ Internal Snapshot JSON

        Returns:
            dict: {
                "potential_signals": list[dict],  # ì‹œê·¸ë„ í›„ë³´
                "risk_indicators": list[dict],    # ë¦¬ìŠ¤í¬ ì§€í‘œ
                "analysis_summary": str           # ë¶„ì„ ìš”ì•½
            }
        """
        pass

    @abstractmethod
    def extract_document_facts(
        self,
        image_base64: str,
        doc_type: str,
        system_prompt: str,
        user_prompt: str,
    ) -> dict:
        """
        ë¬¸ì„œ ì´ë¯¸ì§€ì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ (Vision)

        Args:
            image_base64: Base64 ì¸ì½”ë”©ëœ ë¬¸ì„œ ì´ë¯¸ì§€
            doc_type: ë¬¸ì„œ íƒ€ì… (BIZ_REG, REGISTRY, etc.)
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            user_prompt: ë¬¸ì„œ íƒ€ì…ë³„ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸

        Returns:
            dict: ì¶”ì¶œëœ êµ¬ì¡°í™” ì •ë³´ (PRD 14.4 fact ìŠ¤í‚¤ë§ˆ)
        """
        pass

    @abstractmethod
    def generate_signals(
        self,
        internal_context: dict,
        external_intel: list[dict],
        system_prompt: str,
        user_prompt: str,
    ) -> list[dict]:
        """
        Internal + External ê²°í•©í•˜ì—¬ ìµœì¢… ì‹œê·¸ë„ ìƒì„±

        Args:
            internal_context: ë‚´ë¶€ ë°ì´í„° (Snapshot + Doc Facts)
            external_intel: External LLMì´ ì ì¬í•œ ì™¸ë¶€ ì¸í…”ë¦¬ì „ìŠ¤
            system_prompt: ì‹œê·¸ë„ ì¶”ì¶œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            user_prompt: í¬ë§·ëœ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸

        Returns:
            list[dict]: ì‹œê·¸ë„ ëª©ë¡
        """
        pass

    @abstractmethod
    def validate_signal(self, signal: dict) -> dict:
        """
        ì‹œê·¸ë„ ê²€ì¦ (ê·¼ê±° í™•ì¸, í‘œí˜„ ê²€í† )

        Args:
            signal: ê²€ì¦í•  ì‹œê·¸ë„

        Returns:
            dict: ê²€ì¦ëœ ì‹œê·¸ë„ (ë˜ëŠ” ìˆ˜ì •ëœ ì‹œê·¸ë„)
        """
        pass

    @abstractmethod
    def generate_insight(
        self,
        signals: list[dict],
        context: dict,
        similar_cases: list[dict] = None,
    ) -> str:
        """
        ìµœì¢… ì¸ì‚¬ì´íŠ¸ ìš”ì•½ ìƒì„±

        Args:
            signals: ê²€ì¦ëœ ì‹œê·¸ë„ ëª©ë¡
            context: í†µí•© ì»¨í…ìŠ¤íŠ¸
            similar_cases: ìœ ì‚¬ ê³¼ê±° ì¼€ì´ìŠ¤ (ë²¡í„° ê²€ìƒ‰ ê²°ê³¼)

        Returns:
            str: ìƒì„±ëœ ì¸ì‚¬ì´íŠ¸ ìš”ì•½ (í•œêµ­ì–´)
        """
        pass


class MVPInternalLLM(InternalLLMBase):
    """
    Phase 1: MVPìš© Internal LLM êµ¬í˜„

    ì‹¤ì œë¡œëŠ” ì™¸ë¶€ APIë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ,
    InternalLLMBase ì¸í„°í˜ì´ìŠ¤ë¡œ ì¶”ìƒí™”

    âš ï¸ MVP ì œì•½:
    - ì‹¤ ê³ ê° ë°ì´í„° ì‚¬ìš© ê¸ˆì§€ (ê°€ìƒ ë°ì´í„°ë§Œ)
    - ëŒ€íšŒ ì‹œì—°ìš©ìœ¼ë¡œë§Œ í™œìš©
    """

    # MVP ëª¨ë¸ ì„¤ì • (ì €ë¹„ìš© + ë¹ ë¥¸ ì‘ë‹µ)
    TEXT_MODELS = [
        {"model": "gpt-3.5-turbo", "provider": "openai"},
        {"model": "claude-3-haiku-20240307", "provider": "anthropic"},
    ]

    # Vision ëª¨ë¸ (ë¬¸ì„œ OCR)
    VISION_MODELS = [
        {"model": "gpt-4o-mini", "provider": "openai"},
        {"model": "claude-sonnet-4-20250514", "provider": "anthropic"},
    ]

    MAX_RETRIES = 3
    INITIAL_DELAY = 1.0
    MAX_DELAY = 30.0
    BACKOFF_MULTIPLIER = 2.0

    def __init__(self):
        self._provider = InternalLLMProvider.MVP_OPENAI
        self._model_name = "gpt-3.5-turbo"
        self._configure_api_keys()

    @property
    def provider(self) -> InternalLLMProvider:
        return self._provider

    @property
    def model_name(self) -> str:
        return self._model_name

    def _configure_api_keys(self):
        """API í‚¤ ì„¤ì •"""
        # Internal LLMìš© ë³„ë„ í‚¤ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ í‚¤ ì‚¬ìš©
        internal_openai_key = os.getenv("INTERNAL_LLM_OPENAI_KEY", settings.OPENAI_API_KEY)
        internal_anthropic_key = os.getenv("INTERNAL_LLM_ANTHROPIC_KEY", settings.ANTHROPIC_API_KEY)

        if internal_openai_key:
            litellm.openai_key = internal_openai_key
        if internal_anthropic_key:
            litellm.anthropic_key = internal_anthropic_key

    def _call_with_retry(
        self,
        messages: list[dict],
        models: list[dict],
        response_format: Optional[dict] = None,
        temperature: float = 0.1,
        max_tokens: int = 2048,
    ) -> str:
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ LLM í˜¸ì¶œ"""
        errors = []

        for model_config in models:
            model = model_config["model"]
            provider = model_config["provider"]

            for attempt in range(self.MAX_RETRIES):
                try:
                    logger.info(f"[Internal LLM] Calling {model} (attempt {attempt + 1})")

                    kwargs = {
                        "model": model,
                        "messages": messages,
                        "temperature": temperature,
                        "max_tokens": max_tokens,
                    }

                    if response_format:
                        kwargs["response_format"] = response_format

                    response = completion(**kwargs)
                    content = response.choices[0].message.content

                    self._model_name = model
                    logger.info(f"[Internal LLM] Success from {model}")

                    return content

                except Exception as e:
                    errors.append({"model": model, "error": str(e)})
                    logger.warning(f"[Internal LLM] {model} failed: {e}")

                    if attempt < self.MAX_RETRIES - 1:
                        delay = min(
                            self.INITIAL_DELAY * (self.BACKOFF_MULTIPLIER ** attempt),
                            self.MAX_DELAY,
                        )
                        time.sleep(delay)

        raise AllProvidersFailedError(
            message=f"[Internal LLM] All providers failed",
            errors=errors,
        )

    def analyze_snapshot(self, snapshot_json: dict) -> dict:
        """
        [MVP êµ¬í˜„] Snapshot ë¶„ì„í•˜ì—¬ ë¦¬ìŠ¤í¬ ì§€í‘œ ì¶”ì¶œ
        """
        system_prompt = """You are a senior risk analyst at a Korean bank.
Analyze the internal snapshot data and identify potential risk indicators.
Focus on:
1. KYC status changes
2. Credit grade changes
3. Loan exposure changes
4. Collateral changes
5. Overdue flags

Return JSON with structure:
{
    "potential_signals": [{"type": "...", "reason": "..."}],
    "risk_indicators": [{"indicator": "...", "severity": "HIGH/MED/LOW"}],
    "analysis_summary": "..."
}
"""
        user_prompt = f"""Analyze this internal snapshot data:

```json
{json.dumps(snapshot_json, ensure_ascii=False, indent=2)}
```

Identify any risk indicators or potential signal triggers.
Respond in Korean for analysis_summary."""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        result = self._call_with_retry(
            messages=messages,
            models=self.TEXT_MODELS,
            response_format={"type": "json_object"},
        )

        try:
            return json.loads(result)
        except json.JSONDecodeError:
            return {
                "potential_signals": [],
                "risk_indicators": [],
                "analysis_summary": result[:500],
            }

    def extract_document_facts(
        self,
        image_base64: str,
        doc_type: str,
        system_prompt: str,
        user_prompt: str,
    ) -> dict:
        """
        [MVP êµ¬í˜„] Vision LLMìœ¼ë¡œ ë¬¸ì„œì—ì„œ ì •ë³´ ì¶”ì¶œ
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{image_base64}",
                            "detail": "high",
                        },
                    },
                    {"type": "text", "text": user_prompt},
                ],
            },
        ]

        result = self._call_with_retry(
            messages=messages,
            models=self.VISION_MODELS,
            max_tokens=4096,
        )

        try:
            return json.loads(result)
        except json.JSONDecodeError as e:
            raise InvalidResponseError(
                message=f"Failed to parse document extraction: {e}",
                raw_response=result[:500],
            )

    def generate_signals(
        self,
        internal_context: dict,
        external_intel: list[dict],
        system_prompt: str,
        user_prompt: str,
    ) -> list[dict]:
        """
        [MVP êµ¬í˜„] ë‚´ë¶€+ì™¸ë¶€ ë°ì´í„° ê²°í•©í•˜ì—¬ ì‹œê·¸ë„ ìƒì„±
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        result = self._call_with_retry(
            messages=messages,
            models=self.TEXT_MODELS,
            response_format={"type": "json_object"},
            max_tokens=4096,
        )

        try:
            parsed = json.loads(result)
            signals = parsed.get("signals", [])
            logger.info(f"[Internal LLM] Extracted {len(signals)} signals")
            return signals
        except json.JSONDecodeError:
            logger.error("[Internal LLM] Failed to parse signals response")
            return []

    def validate_signal(self, signal: dict) -> dict:
        """
        [MVP êµ¬í˜„] ì‹œê·¸ë„ ê²€ì¦ (ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜)
        """
        # MVPì—ì„œëŠ” ê¸°ì¡´ ValidationPipelineì— ìœ„ì„
        # Phase 2/3ì—ì„œ LLM ê¸°ë°˜ ê²€ì¦ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥
        return signal

    def generate_insight(
        self,
        signals: list[dict],
        context: dict,
        similar_cases: list[dict] = None,
    ) -> str:
        """
        [MVP êµ¬í˜„] ì¸ì‚¬ì´íŠ¸ ìš”ì•½ ìƒì„±
        """
        from app.worker.llm.prompts import INSIGHT_GENERATION_PROMPT

        # ì‹œê·¸ë„ ìš”ì•½ ë¹Œë“œ
        signal_summary = self._build_signal_summary(signals)

        user_prompt = INSIGHT_GENERATION_PROMPT.format(
            corp_name=context.get("corp_name", ""),
            industry_code=context.get("industry_code", ""),
            industry_name=context.get("industry_name", ""),
            signal_count=len(signals),
            signals_summary=signal_summary,
        )

        # ìœ ì‚¬ ì¼€ì´ìŠ¤ ì¶”ê°€
        if similar_cases:
            cases_text = self._format_similar_cases(similar_cases)
            user_prompt += f"\n\n### ìœ ì‚¬ ê³¼ê±° ì¼€ì´ìŠ¤ ì°¸ê³ \n{cases_text}"

        messages = [
            {
                "role": "system",
                "content": (
                    "You are a senior risk analyst providing executive briefings. "
                    "Generate concise, actionable insights in Korean. "
                    "Use probabilistic language: '~ë¡œ ì¶”ì •ë¨', '~ê°€ëŠ¥ì„± ìˆìŒ', 'ê²€í†  ê¶Œê³ '. "
                    "Avoid definitive statements."
                ),
            },
            {"role": "user", "content": user_prompt},
        ]

        return self._call_with_retry(
            messages=messages,
            models=self.TEXT_MODELS,
            temperature=0.3,
            max_tokens=1000,
        ).strip()

    def _build_signal_summary(self, signals: list[dict]) -> str:
        """ì‹œê·¸ë„ ìš”ì•½ í…ìŠ¤íŠ¸ ìƒì„±"""
        lines = []

        risk_signals = [s for s in signals if s.get("impact_direction") == "RISK"]
        opp_signals = [s for s in signals if s.get("impact_direction") == "OPPORTUNITY"]

        if risk_signals:
            lines.append(f"## ë¦¬ìŠ¤í¬ ì‹œê·¸ë„ ({len(risk_signals)}ê±´)")
            for s in risk_signals:
                strength = s.get("impact_strength", "MED")
                lines.append(f"- [{strength}] {s.get('title', '')}: {s.get('summary', '')[:100]}")

        if opp_signals:
            lines.append(f"\n## ê¸°íšŒ ì‹œê·¸ë„ ({len(opp_signals)}ê±´)")
            for s in opp_signals:
                strength = s.get("impact_strength", "MED")
                lines.append(f"- [{strength}] {s.get('title', '')}: {s.get('summary', '')[:100]}")

        return "\n".join(lines)

    def _format_similar_cases(self, cases: list[dict]) -> str:
        """ìœ ì‚¬ ì¼€ì´ìŠ¤ í¬ë§·íŒ…"""
        lines = []
        for i, case in enumerate(cases, 1):
            similarity_pct = int(case.get("similarity", 0) * 100)
            lines.append(
                f"{i}. [{case.get('signal_type', 'N/A')}] {case.get('event_type', 'N/A')} "
                f"(ìœ ì‚¬ë„ {similarity_pct}%)\n"
                f"   - ì—…ì¢…: {case.get('industry_code', 'N/A')}\n"
                f"   - ìš”ì•½: {case.get('summary', 'N/A')[:100]}..."
            )
        return "\n".join(lines)


class AzureInternalLLM(InternalLLMBase):
    """
    Phase 2: Azure OpenAI ê¸°ë°˜ Internal LLM

    ë°ì´í„°ê°€ Azure êµ­ë‚´ ë¦¬ì „ì—ì„œë§Œ ì²˜ë¦¬ë¨
    (ì¶”í›„ êµ¬í˜„ ì˜ˆì •)
    """

    def __init__(self):
        self._provider = InternalLLMProvider.AZURE_OPENAI
        self._model_name = "gpt-4"
        # Azure ì„¤ì •ì€ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ
        self._endpoint = os.getenv("INTERNAL_LLM_AZURE_ENDPOINT", "")
        self._deployment = os.getenv("INTERNAL_LLM_AZURE_DEPLOYMENT", "gpt-4")
        self._api_version = os.getenv("INTERNAL_LLM_AZURE_API_VERSION", "2024-02-15-preview")

    @property
    def provider(self) -> InternalLLMProvider:
        return self._provider

    @property
    def model_name(self) -> str:
        return self._model_name

    def analyze_snapshot(self, snapshot_json: dict) -> dict:
        raise NotImplementedError("Phase 2 - Azure OpenAI implementation pending")

    def extract_document_facts(self, image_base64: str, doc_type: str,
                               system_prompt: str, user_prompt: str) -> dict:
        raise NotImplementedError("Phase 2 - Azure OpenAI implementation pending")

    def generate_signals(self, internal_context: dict, external_intel: list[dict],
                         system_prompt: str, user_prompt: str) -> list[dict]:
        raise NotImplementedError("Phase 2 - Azure OpenAI implementation pending")

    def validate_signal(self, signal: dict) -> dict:
        raise NotImplementedError("Phase 2 - Azure OpenAI implementation pending")

    def generate_insight(self, signals: list[dict], context: dict,
                         similar_cases: list[dict] = None) -> str:
        raise NotImplementedError("Phase 2 - Azure OpenAI implementation pending")


class OnPremLlamaLLM(InternalLLMBase):
    """
    Phase 3: On-Premise Llama 3 ê¸°ë°˜ Internal LLM

    ì™„ì „ ìì²´ ì¸í”„ë¼ì—ì„œ ìš´ì˜
    (ì¶”í›„ êµ¬í˜„ ì˜ˆì •)
    """

    def __init__(self):
        self._provider = InternalLLMProvider.ONPREM_LLAMA
        self._model_name = "meta-llama/Llama-3.1-70B-Instruct"
        # vLLM ì„œë²„ ì—”ë“œí¬ì¸íŠ¸
        self._endpoint = os.getenv("INTERNAL_LLM_ONPREM_ENDPOINT", "http://internal-llm.bank.local:8000")

    @property
    def provider(self) -> InternalLLMProvider:
        return self._provider

    @property
    def model_name(self) -> str:
        return self._model_name

    def analyze_snapshot(self, snapshot_json: dict) -> dict:
        raise NotImplementedError("Phase 3 - On-Premise Llama implementation pending")

    def extract_document_facts(self, image_base64: str, doc_type: str,
                               system_prompt: str, user_prompt: str) -> dict:
        raise NotImplementedError("Phase 3 - On-Premise Llama implementation pending")

    def generate_signals(self, internal_context: dict, external_intel: list[dict],
                         system_prompt: str, user_prompt: str) -> list[dict]:
        raise NotImplementedError("Phase 3 - On-Premise Llama implementation pending")

    def validate_signal(self, signal: dict) -> dict:
        raise NotImplementedError("Phase 3 - On-Premise Llama implementation pending")

    def generate_insight(self, signals: list[dict], context: dict,
                         similar_cases: list[dict] = None) -> str:
        raise NotImplementedError("Phase 3 - On-Premise Llama implementation pending")


def get_internal_llm() -> InternalLLMBase:
    """
    Factory: í™˜ê²½ ë³€ìˆ˜ì— ë”°ë¼ Internal LLM ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜

    INTERNAL_LLM_PROVIDER í™˜ê²½ë³€ìˆ˜ë¡œ ì œì–´:
    - mvp_openai (ê¸°ë³¸ê°’) â†’ MVPInternalLLM
    - azure_openai â†’ AzureInternalLLM
    - onprem_llama â†’ OnPremLlamaLLM
    """
    provider = os.getenv("INTERNAL_LLM_PROVIDER", "mvp_openai").lower()

    if provider == "mvp_openai" or provider == "mvp_anthropic":
        return MVPInternalLLM()
    elif provider == "azure_openai":
        return AzureInternalLLM()
    elif provider == "onprem_llama":
        return OnPremLlamaLLM()
    else:
        logger.warning(f"Unknown Internal LLM provider: {provider}, using MVP")
        return MVPInternalLLM()
