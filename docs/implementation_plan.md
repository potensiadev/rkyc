# rKYC Implementation Plan: ì½”ë“œë² ì´ìŠ¤ ë¶ˆì¼ì¹˜ ì‚¬í•­ í•´ê²°

## ê°œìš”

í˜„ì¬ ì½”ë“œë² ì´ìŠ¤ì™€ PRD/ì•„í‚¤í…ì²˜ ë¬¸ì„œ ê°„ ë¶ˆì¼ì¹˜ ì‚¬í•­ì„ í•´ê²°í•˜ê¸° ìœ„í•œ êµ¬í˜„ ê³„íšì„œì…ë‹ˆë‹¤.
Claude Codeê°€ ìˆœì°¨ì ìœ¼ë¡œ ì‘ì—…í•  ìˆ˜ ìˆë„ë¡ ìš°ì„ ìˆœìœ„ì™€ ìƒì„¸ íƒœìŠ¤í¬ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

---

## ë¶ˆì¼ì¹˜ í•­ëª© ìš”ì•½

| # | í•­ëª© | í˜„ì¬ ìƒíƒœ | ëª©í‘œ ìƒíƒœ | ìš°ì„ ìˆœìœ„ |
|---|------|----------|----------|----------|
| 1 | DOC_INGEST | Vision LLM | pdfplumber + ì •ê·œì‹ + LLM ë³´ì™„ | ğŸ”´ P0 |
| 2 | LLM Fallback | 2ë‹¨ê³„ (Claude â†’ GPT-4o) | 3ë‹¨ê³„ (+ Gemini 1.5 Pro) | ğŸŸ¡ P1 |
| 3 | Embedding/pgvector | íŒŒì¼ë§Œ ì¡´ì¬, ë¯¸ì‚¬ìš© | ì¸ì‚¬ì´íŠ¸ ë©”ëª¨ë¦¬ ë²¡í„° ê²€ìƒ‰ | ğŸŸ¢ P2 |
| 4 | Celery Worker | ì„¤ì •ë§Œ ì¡´ì¬ | Railway ë°°í¬ ë˜ëŠ” ë™ê¸° ì‹¤í–‰ í™•ì¸ | ğŸŸ¢ P2 |

---

## Task 1: DOC_INGEST íŒŒì´í”„ë¼ì¸ ì¬êµ¬í˜„ (P0)

### 1.1 ëª©í‘œ
Vision LLM ê¸°ë°˜ ë¬¸ì„œ ì²˜ë¦¬ë¥¼ **PDF í…ìŠ¤íŠ¸ íŒŒì‹± + ì •ê·œì‹ + LLM ë³´ì™„** ë°©ì‹ìœ¼ë¡œ ë³€ê²½

### 1.2 ë³€ê²½ ì´ìœ 
- ë¹„ìš© ì ˆê°: Vision LLM ëŒ€ë¹„ 1/10 ì´í•˜
- ì†ë„ í–¥ìƒ: ì •ê·œì‹ì€ ë°€ë¦¬ì´ˆ ë‹¨ìœ„
- ì •í™•ë„: ì •í˜•í™”ëœ KYC ë¬¸ì„œëŠ” ê·œì¹™ ê¸°ë°˜ì´ ë” ì¼ê´€ë¨

### 1.3 íŒŒì¼ ë³€ê²½ ëª©ë¡

```
backend/
â”œâ”€â”€ requirements.txt                    # pdfplumber ì¶”ê°€
â”œâ”€â”€ app/worker/pipelines/
â”‚   â”œâ”€â”€ doc_ingest.py                  # ì „ë©´ ì¬ì‘ì„±
â”‚   â””â”€â”€ doc_parsers/                   # ì‹ ê·œ ë””ë ‰í† ë¦¬
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ base.py                    # ë² ì´ìŠ¤ íŒŒì„œ í´ë˜ìŠ¤
â”‚       â”œâ”€â”€ biz_reg_parser.py          # ì‚¬ì—…ìë“±ë¡ì¦ íŒŒì„œ
â”‚       â”œâ”€â”€ registry_parser.py         # ë“±ê¸°ë¶€ë“±ë³¸ íŒŒì„œ
â”‚       â”œâ”€â”€ shareholders_parser.py     # ì£¼ì£¼ëª…ë¶€ íŒŒì„œ
â”‚       â”œâ”€â”€ aoi_parser.py              # ì •ê´€ íŒŒì„œ
â”‚       â””â”€â”€ fin_statement_parser.py    # ì¬ë¬´ì œí‘œ íŒŒì„œ
â””â”€â”€ app/worker/llm/
    â””â”€â”€ prompts.py                     # DOC_EXTRACTION í”„ë¡¬í”„íŠ¸ ìˆ˜ì •
```

### 1.4 êµ¬í˜„ ìƒì„¸

#### Step 1: requirements.txt ìˆ˜ì •
```txt
# Document Parsing
pdfplumber>=0.10.0
```

#### Step 2: ë² ì´ìŠ¤ íŒŒì„œ í´ë˜ìŠ¤ ìƒì„±
```python
# backend/app/worker/pipelines/doc_parsers/base.py

from abc import ABC, abstractmethod
from typing import Optional
import pdfplumber
import re

class BaseDocParser(ABC):
    """ë¬¸ì„œ íŒŒì„œ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, llm_service=None):
        self.llm = llm_service
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        text_content = []
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                text = page.extract_text()
                if text:
                    text_content.append(text)
        return "\n".join(text_content)
    
    def extract_tables_from_pdf(self, pdf_path: str) -> list[list]:
        """PDFì—ì„œ í‘œ ì¶”ì¶œ"""
        tables = []
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                page_tables = page.extract_tables()
                if page_tables:
                    tables.extend(page_tables)
        return tables
    
    @abstractmethod
    def parse(self, pdf_path: str) -> dict:
        """ë¬¸ì„œ íŒŒì‹± - í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„"""
        pass
    
    @abstractmethod
    def get_regex_patterns(self) -> dict:
        """ì •ê·œì‹ íŒ¨í„´ ë°˜í™˜ - í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„"""
        pass
    
    def extract_with_regex(self, text: str, patterns: dict) -> dict:
        """ì •ê·œì‹ìœ¼ë¡œ í•„ë“œ ì¶”ì¶œ"""
        results = {}
        failed_fields = []
        
        for field_name, pattern in patterns.items():
            match = re.search(pattern, text, re.MULTILINE)
            if match:
                results[field_name] = match.group(1).strip()
            else:
                failed_fields.append(field_name)
        
        return results, failed_fields
    
    def fallback_to_llm(self, text: str, failed_fields: list, doc_type: str) -> dict:
        """ì‹¤íŒ¨í•œ í•„ë“œë§Œ LLMìœ¼ë¡œ ë³´ì™„"""
        if not failed_fields or not self.llm:
            return {}
        
        prompt = f"""ë‹¤ìŒ {doc_type} ë¬¸ì„œì—ì„œ ì•„ë˜ í•„ë“œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
        
ì¶”ì¶œí•  í•„ë“œ: {', '.join(failed_fields)}

ë¬¸ì„œ ë‚´ìš©:
{text[:3000]}

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{"field_name": "value", ...}}

ì°¾ì„ ìˆ˜ ì—†ëŠ” í•„ë“œëŠ” nullë¡œ í‘œì‹œí•˜ì„¸ìš”."""

        messages = [{"role": "user", "content": prompt}]
        result = self.llm.call_with_json_response(messages)
        return result
```

#### Step 3: ì‚¬ì—…ìë“±ë¡ì¦ íŒŒì„œ êµ¬í˜„ (ì˜ˆì‹œ)
```python
# backend/app/worker/pipelines/doc_parsers/biz_reg_parser.py

from .base import BaseDocParser

class BizRegParser(BaseDocParser):
    """ì‚¬ì—…ìë“±ë¡ì¦ íŒŒì„œ"""
    
    DOC_TYPE = "BIZ_REG"
    
    def get_regex_patterns(self) -> dict:
        return {
            "biz_no": r"ì‚¬ì—…ì\s*ë“±ë¡ë²ˆí˜¸[:\s]*(\d{3}-\d{2}-\d{5})",
            "corp_name": r"ìƒ\s*í˜¸[:\s]*(.+?)(?:\n|ë²•ì¸ëª…)",
            "ceo_name": r"ëŒ€\s*í‘œ\s*ì[:\s]*(.+?)(?:\n|ì£¼ë¯¼)",
            "address": r"ì‚¬ì—…ì¥\s*ì†Œì¬ì§€[:\s]*(.+?)(?:\n|ì—…)",
            "biz_type": r"ì—…\s*íƒœ[:\s]*(.+?)(?:\n|ì¢…ëª©)",
            "biz_item": r"ì¢…\s*ëª©[:\s]*(.+?)(?:\n|$)",
            "open_date": r"ê°œì—…\s*ì—°ì›”ì¼[:\s]*(\d{4}[.\-/]\d{2}[.\-/]\d{2})",
        }
    
    def parse(self, pdf_path: str) -> dict:
        """ì‚¬ì—…ìë“±ë¡ì¦ íŒŒì‹±"""
        # Step 1: í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = self.extract_text_from_pdf(pdf_path)
        
        # Step 2: ì •ê·œì‹ìœ¼ë¡œ ì¶”ì¶œ
        patterns = self.get_regex_patterns()
        results, failed_fields = self.extract_with_regex(text, patterns)
        
        # Step 3: ì‹¤íŒ¨í•œ í•„ë“œë§Œ LLM ë³´ì™„
        if failed_fields:
            llm_results = self.fallback_to_llm(text, failed_fields, "ì‚¬ì—…ìë“±ë¡ì¦")
            results.update({k: v for k, v in llm_results.items() if v is not None})
        
        # Step 4: fact í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        facts = []
        for field_key, field_value in results.items():
            facts.append({
                "fact_type": "BIZ_REG",
                "field_key": field_key,
                "field_value": field_value,
                "confidence": "HIGH" if field_key not in failed_fields else "MED",
                "evidence_snippet": self._get_snippet(text, field_value),
            })
        
        return {"facts": facts, "raw_text": text[:500]}
    
    def _get_snippet(self, text: str, value: str, context_chars: int = 50) -> str:
        """ê°’ ì£¼ë³€ í…ìŠ¤íŠ¸ ìŠ¤ë‹ˆí« ì¶”ì¶œ"""
        if not value or value not in text:
            return ""
        idx = text.find(value)
        start = max(0, idx - context_chars)
        end = min(len(text), idx + len(value) + context_chars)
        return text[start:end]
```

#### Step 4: doc_ingest.py ìˆ˜ì •
```python
# backend/app/worker/pipelines/doc_ingest.py ì£¼ìš” ë³€ê²½ì‚¬í•­

from app.worker.pipelines.doc_parsers import (
    BizRegParser,
    RegistryParser,
    ShareholdersParser,
    AoiParser,
    FinStatementParser,
)

class DocIngestPipeline:
    """Stage 2: DOC_INGEST - PDF í…ìŠ¤íŠ¸ íŒŒì‹± ê¸°ë°˜ ë¬¸ì„œ ì²˜ë¦¬"""
    
    def __init__(self):
        self.llm = LLMService()
        
        # ë¬¸ì„œ íƒ€ì…ë³„ íŒŒì„œ ë§¤í•‘
        self.parsers = {
            DocType.BIZ_REG: BizRegParser(self.llm),
            DocType.REGISTRY: RegistryParser(self.llm),
            DocType.SHAREHOLDERS: ShareholdersParser(self.llm),
            DocType.AOI: AoiParser(self.llm),
            DocType.FIN_STATEMENT: FinStatementParser(self.llm),
        }
    
    def _process_document(self, db, doc: Document, corp_id: str) -> Optional[dict]:
        """ë‹¨ì¼ ë¬¸ì„œ ì²˜ë¦¬ - í…ìŠ¤íŠ¸ íŒŒì‹± ë°©ì‹"""
        parser = self.parsers.get(doc.doc_type)
        if not parser:
            raise DocumentProcessingError(f"No parser for doc_type: {doc.doc_type}")
        
        # PDF íŒŒì‹± ì‹¤í–‰
        result = parser.parse(doc.storage_path)
        
        # facts ì €ì¥
        saved_facts = self._save_facts(db, doc, corp_id, result.get("facts", []))
        
        return {
            "doc_type": doc.doc_type.value,
            "facts": saved_facts,
            "extraction_method": "pdf_parser",
        }
```

### 1.5 í…ŒìŠ¤íŠ¸ ê³„íš
```bash
# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
pytest backend/tests/test_doc_parsers.py -v

# í†µí•© í…ŒìŠ¤íŠ¸ - ì‹¤ì œ PDF íŒŒì¼ë¡œ í…ŒìŠ¤íŠ¸
python -m app.worker.pipelines.doc_ingest --test --corp_id=TEST001
```

---

## Task 2: LLM Fallback 3ë‹¨ê³„ í™•ì¥ (P1)

### 2.1 ëª©í‘œ
í˜„ì¬ 2ë‹¨ê³„ fallback (Claude â†’ GPT-4o)ì„ 3ë‹¨ê³„ (+ Gemini 1.5 Pro)ë¡œ í™•ì¥

### 2.2 íŒŒì¼ ë³€ê²½ ëª©ë¡
```
backend/
â”œâ”€â”€ .env.example                       # GOOGLE_API_KEY ì¶”ê°€
â”œâ”€â”€ app/core/config.py                 # GOOGLE_API_KEY ì„¤ì • ì¶”ê°€
â””â”€â”€ app/worker/llm/service.py          # Gemini ëª¨ë¸ ì¶”ê°€
```

### 2.3 êµ¬í˜„ ìƒì„¸

#### Step 1: config.py ìˆ˜ì •
```python
# backend/app/core/config.py ì¶”ê°€

class Settings(BaseSettings):
    # ... ê¸°ì¡´ ì„¤ì • ...
    
    # LLM API Keys
    ANTHROPIC_API_KEY: str = ""
    OPENAI_API_KEY: str = ""
    GOOGLE_API_KEY: str = ""  # ì¶”ê°€
```

#### Step 2: service.py ìˆ˜ì •
```python
# backend/app/worker/llm/service.py

class LLMService:
    # Model configuration - 3ë‹¨ê³„ fallback
    MODELS = [
        {
            "model": "claude-opus-4-5-20251101",
            "provider": "anthropic",
            "max_tokens": 4096,
        },
        {
            "model": "gpt-5.2-pro-2025-12-11",
            "provider": "openai",
            "max_tokens": 4096,
        },
        {
            "model": "gemini/gemini-3-pro-preview",
            "provider": "google",
            "max_tokens": 4096,
        },
    ]
    
    def _configure_api_keys(self):
        """Set API keys for litellm"""
        if settings.ANTHROPIC_API_KEY:
            litellm.anthropic_key = settings.ANTHROPIC_API_KEY
        if settings.OPENAI_API_KEY:
            litellm.openai_key = settings.OPENAI_API_KEY
        if settings.GOOGLE_API_KEY:  # ì¶”ê°€
            litellm.google_key = settings.GOOGLE_API_KEY
    
    def _get_api_key(self, provider: str) -> str:
        """Get API key for specific provider"""
        if provider == "anthropic":
            return settings.ANTHROPIC_API_KEY
        elif provider == "openai":
            return settings.OPENAI_API_KEY
        elif provider == "google":  # ì¶”ê°€
            return settings.GOOGLE_API_KEY
        return ""
```

### 2.4 í…ŒìŠ¤íŠ¸ ê³„íš
```python
# ê° providerë³„ ê°œë³„ í…ŒìŠ¤íŠ¸
def test_claude_fallback():
    # Claude API keyë¥¼ ì„ì‹œë¡œ ë¬´íš¨í™”í•˜ê³  GPT-4oë¡œ fallback í™•ì¸
    pass

def test_gpt_fallback():
    # Claude, GPT ë‘˜ ë‹¤ ë¬´íš¨í™”í•˜ê³  Geminië¡œ fallback í™•ì¸
    pass
```

---

## Task 3: Embedding & pgvector ì¸ì‚¬ì´íŠ¸ ë©”ëª¨ë¦¬ êµ¬í˜„ (P2)

### 3.1 ëª©í‘œ
ì¸ì‚¬ì´íŠ¸ ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì„ ìœ„í•œ ë²¡í„° ê²€ìƒ‰ êµ¬í˜„

### 3.2 íŒŒì¼ ë³€ê²½ ëª©ë¡
```
backend/
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ migration_v5_vector.sql        # ì´ë¯¸ ì¡´ì¬, ì ìš© í™•ì¸
â”œâ”€â”€ app/worker/llm/embedding.py        # êµ¬í˜„ ì™„ì„±
â””â”€â”€ app/worker/pipelines/insight.py    # ë²¡í„° ê²€ìƒ‰ ì—°ë™
```

### 3.3 êµ¬í˜„ ìƒì„¸

#### Step 1: migration ì ìš© í™•ì¸
```sql
-- Supabaseì—ì„œ pgvector í™•ì¥ í™œì„±í™” í™•ì¸
CREATE EXTENSION IF NOT EXISTS vector;

-- rkyc_case_indexì— embedding ì»¬ëŸ¼ ì¶”ê°€ (migration_v5 ì°¸ì¡°)
ALTER TABLE rkyc_case_index 
ADD COLUMN IF NOT EXISTS embedding vector(1536);

-- ë²¡í„° ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒì„±
CREATE INDEX IF NOT EXISTS idx_case_embedding 
ON rkyc_case_index 
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);
```

#### Step 2: embedding.py ì™„ì„±
```python
# backend/app/worker/llm/embedding.py

import numpy as np
from openai import OpenAI
from app.core.config import settings

class EmbeddingService:
    """OpenAI Embedding Service"""

    MODEL = "text-embedding-3-large"
    DIMENSIONS = 2000
    
    def __init__(self):
        self.client = OpenAI(api_key=settings.OPENAI_API_KEY)
    
    def embed_text(self, text: str) -> list[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        response = self.client.embeddings.create(
            model=self.MODEL,
            input=text,
            dimensions=self.DIMENSIONS,
        )
        return response.data[0].embedding
    
    def embed_signal(self, signal: dict) -> list[float]:
        """ì‹œê·¸ë„ì„ ë²¡í„°ë¡œ ë³€í™˜"""
        # ì‹œê·¸ë„ ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
        text = f"""
        Signal Type: {signal.get('signal_type', '')}
        Event Type: {signal.get('event_type', '')}
        Industry: {signal.get('industry_code', '')}
        Summary: {signal.get('summary', '')}
        """
        return self.embed_text(text)
```

#### Step 3: insight.py ë²¡í„° ê²€ìƒ‰ ì—°ë™
```python
# backend/app/worker/pipelines/insight.py ìˆ˜ì •

from app.worker.llm.embedding import EmbeddingService

class InsightPipeline:
    def __init__(self):
        self.embedding = EmbeddingService()
    
    def find_similar_cases(self, signal: dict, top_k: int = 5) -> list[dict]:
        """ìœ ì‚¬ ì¼€ì´ìŠ¤ ë²¡í„° ê²€ìƒ‰"""
        # í˜„ì¬ ì‹œê·¸ë„ ì„ë² ë”©
        query_embedding = self.embedding.embed_signal(signal)
        
        # pgvector ìœ ì‚¬ë„ ê²€ìƒ‰
        with get_sync_db() as db:
            result = db.execute(
                text("""
                    SELECT 
                        case_id, corp_id, signal_type, event_type,
                        summary, evidence_refs,
                        1 - (embedding <=> :query_embedding) as similarity
                    FROM rkyc_case_index
                    WHERE embedding IS NOT NULL
                    ORDER BY embedding <=> :query_embedding
                    LIMIT :top_k
                """),
                {"query_embedding": str(query_embedding), "top_k": top_k}
            )
            return [dict(row) for row in result.fetchall()]
```

---

## Task 4: Worker ë°°í¬ í™•ì¸ ë° ì •ë¦¬ (P2)

### 4.1 ëª©í‘œ
Celery Worker ì„¤ì • í™•ì¸ ë° Railway ë°°í¬ ìƒíƒœ ì ê²€

### 4.2 í™•ì¸ ì‚¬í•­

#### Option A: Celery Worker ë³„ë„ ë°°í¬ (ê¶Œì¥)
```toml
# backend/railway-worker.toml (ì‹ ê·œ ìƒì„±)
[build]
builder = "nixpacks"

[deploy]
startCommand = "celery -A app.worker.celery_app worker --loglevel=info"
healthcheckPath = ""
```

Railwayì—ì„œ ë³„ë„ ì„œë¹„ìŠ¤ë¡œ Worker ë°°í¬ í•„ìš”

#### Option B: ë™ê¸° ì‹¤í–‰ (ëŒ€íšŒìš© ê°„ì†Œí™”)
í˜„ì¬ `/jobs/analyze/run` APIê°€ ë™ê¸°ë¡œ ì‹¤í–‰ë˜ëŠ”ì§€ í™•ì¸
```python
# backend/app/api/v1/endpoints/jobs.py í™•ì¸
# Celery task.delay() ëŒ€ì‹  ì§ì ‘ ì‹¤í–‰í•˜ëŠ” ë°©ì‹ì¸ì§€ í™•ì¸
```

### 4.3 í˜„ì¬ ìƒíƒœ íŒŒì•… ëª…ë ¹
```bash
# Railway ì„œë¹„ìŠ¤ ëª©ë¡ í™•ì¸
railway status

# Celery worker í”„ë¡œì„¸ìŠ¤ í™•ì¸
ps aux | grep celery

# Redis ì—°ê²° í™•ì¸
redis-cli ping
```

---

## ì‹¤í–‰ ìˆœì„œ ìš”ì•½

```
Phase 1 (P0) - í•„ìˆ˜
â””â”€â”€ Task 1: DOC_INGEST ì¬êµ¬í˜„
    â”œâ”€â”€ 1.1 requirements.txtì— pdfplumber ì¶”ê°€
    â”œâ”€â”€ 1.2 doc_parsers/ ë””ë ‰í† ë¦¬ ìƒì„±
    â”œâ”€â”€ 1.3 BaseDocParser êµ¬í˜„
    â”œâ”€â”€ 1.4 BizRegParser êµ¬í˜„ (ì‚¬ì—…ìë“±ë¡ì¦)
    â”œâ”€â”€ 1.5 FinStatementParser êµ¬í˜„ (ì¬ë¬´ì œí‘œ)
    â”œâ”€â”€ 1.6 doc_ingest.py ìˆ˜ì •
    â””â”€â”€ 1.7 í…ŒìŠ¤íŠ¸

Phase 2 (P1) - ê¶Œì¥
â””â”€â”€ Task 2: LLM Fallback í™•ì¥
    â”œâ”€â”€ 2.1 config.pyì— GOOGLE_API_KEY ì¶”ê°€
    â”œâ”€â”€ 2.2 service.pyì— Gemini ëª¨ë¸ ì¶”ê°€
    â””â”€â”€ 2.3 í…ŒìŠ¤íŠ¸

Phase 3 (P2) - ì„ íƒ
â”œâ”€â”€ Task 3: Embedding/pgvector êµ¬í˜„
â”‚   â”œâ”€â”€ 3.1 migration ì ìš©
â”‚   â”œâ”€â”€ 3.2 embedding.py ì™„ì„±
â”‚   â””â”€â”€ 3.3 insight.py ë²¡í„° ê²€ìƒ‰ ì—°ë™
â”‚
â””â”€â”€ Task 4: Worker ë°°í¬ í™•ì¸
    â””â”€â”€ 4.1 Railway ë°°í¬ ìƒíƒœ ì ê²€
```

---

## ì˜ˆìƒ ì†Œìš” ì‹œê°„

| Task | ì˜ˆìƒ ì‹œê°„ | ë¹„ê³  |
|------|----------|------|
| Task 1: DOC_INGEST | 4-6ì‹œê°„ | 5ê°œ íŒŒì„œ êµ¬í˜„ í¬í•¨ |
| Task 2: LLM Fallback | 30ë¶„ | ì„¤ì • ì¶”ê°€ë§Œ |
| Task 3: Embedding | 2-3ì‹œê°„ | pgvector ì—°ë™ í¬í•¨ |
| Task 4: Worker | 1ì‹œê°„ | í™•ì¸ ë° ì„¤ì • |
| **Total** | **8-11ì‹œê°„** | |

---

## ì°¸ê³  íŒŒì¼

- PRD: `/docs/prd.md`
- í˜„ì¬ doc_ingest: `/backend/app/worker/pipelines/doc_ingest.py`
- LLM Service: `/backend/app/worker/llm/service.py`
- Embedding (ë¯¸ì™„ì„±): `/backend/app/worker/llm/embedding.py`
- DB Schema: `/backend/sql/schema_v2.sql`
